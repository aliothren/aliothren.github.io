---
---
@article{ren2025_far_attention_replacement,
  title        = {Is Attention Required for Transformer Inference? Explore Function-preserving Attention Replacement},
  author       = {Ren, Yuxin and Collins, Maxwell D. and Hu, Miao and Yang, Huanrui},
  journal      = {arXiv preprint arXiv:2505.21535},
  year         = {2025},
  selected     = {true},
  pdf          = {https://arxiv.org/pdf/2505.21535.pdf},
  doi          = {10.48550/arXiv.2505.21535}
}

@article{wang2025_fier_kv_cache,
  title        = {FIER: Fine-Grained and Efficient KV Cache Retrieval for Long-context LLM Inference},
  author       = {Wang, Dongwei and Liu, Zijie and Wang, Song and Ren, Yuxin and Deng, Jianing and Hu, Jingtong and Chen, Tianlong and Yang, Huanrui},
  journal      = {arXiv preprint arXiv:2508.08256},
  year         = {2025},
  selected     = {false},
  pdf          = {https://arxiv.org/pdf/2508.08256.pdf},
  doi          = {10.48550/arXiv.2508.08256}
}